{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://qiita.com/Hironsan/items/30fe09c85da8a28ebd63"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "<span id=\"一般的な機械学習プロセス\" class=\"fragment\"></span><a href=\"#%E4%B8%80%E8%88%AC%E7%9A%84%E3%81%AA%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E3%83%97%E3%83%AD%E3%82%BB%E3%82%B9\"><i class=\"fa fa-link\"></i></a>一般的な機械学習プロセス</h1>\n",
    "\n",
    "<p>本記事をご覧の方はご存知の通り、「機械学習はアルゴリズムにデータを与えるだけで何か良い結果が出る」というものではありません。性能を出すためには多くの作業が必要です。典型的な作業には、データ収集、データクリーニング、特徴エンジニアリング、モデル選択、ハイパーパラメータチューニング、モデルの評価といった作業が含まれます。下のような図を一度は見たことがあるでしょう。</p>\n",
    "\n",
    "<p><a href=\"https://camo.qiitausercontent.com/1ad7f4bde00d06fb81b35c74c08e92f60041cdb4/68747470733a2f2f64326d787565667165616137736a2e636c6f756466726f6e742e6e65742f735f343733314245454630393438373731374431443442453546324546354538443330413733333743363843303739303038444345423736384631413635344434375f313534313339323834393938315f2b323031382d31312d30352b31332e34302e31332e706e67\" target=\"_blank\" rel=\"nofollow noopener\"><img src=\"https://camo.qiitausercontent.com/1ad7f4bde00d06fb81b35c74c08e92f60041cdb4/68747470733a2f2f64326d787565667165616137736a2e636c6f756466726f6e742e6e65742f735f343733314245454630393438373731374431443442453546324546354538443330413733333743363843303739303038444345423736384631413635344434375f313534313339323834393938315f2b323031382d31312d30352b31332e34302e31332e706e67\" alt=\"一般的な機械学習プロセス\" data-canonical-src=\"https://d2mxuefqeaa7sj.cloudfront.net/s_4731BEEF09487717D1D4BE5F2EF5E8D30A7337C68C079008DCEB768F1A654D47_1541392849981_+2018-11-05+13.40.13.png\"></a><br>\n",
    "出典: <a href=\"https://arxiv.org/abs/1603.06212\" rel=\"nofollow noopener\" target=\"_blank\">Evaluation of a Tree-based Pipeline Optimization Tool for Automating Data Science</a></p>\n",
    "\n",
    "<p>最近よく使われているディープラーニングでも多くの作業が必要なことには変わりありません。確かに、ディープラーニングではモデルが特徴を学習してくれるため、特徴エンジニアリングの労力は減るかもしれません。しかし、プロセス全体は伝統的な機械学習と同様なので、データの前処理やハイパーパラメタのチューニングが必要なことには変わりません。また、高い性能を出すためには、ニューラルネットワークのアーキテクチャ設計に多くの時間を費やす必要があります。</p>\n",
    "\n",
    "\n",
    "\n",
    "<p>このように、機械学習には多くの作業が必要で時間がかかることから自動化技術の重要性が増しています。</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "<span id=\"automlとは\" class=\"fragment\"></span><a href=\"#automl%E3%81%A8%E3%81%AF\"><i class=\"fa fa-link\"></i></a>AutoMLとは？</h1>\n",
    "\n",
    "<p>AutoML（Automated Machine Learning）は、機械学習プロセスの自動化を目的とした技術のことです。機械学習に多くの作業が必要なのは先に述べたとおりですが、AutoMLでは機械学習の各プロセスを自動化してエンジニアの生産性を向上させること、また誰でも機械学習を使えるようになることを目指しています。したがって、究極的な目標は、生データを与えたら、何らかの処理をして、良い結果を出すことだと言えるでしょう。</p>\n",
    "\n",
    "<p>データサイエンティストの数が急激に増えていることもAutoMLを後押しする背景となっています。以下の図はLinkedInでデータサイエンティストの数を調査した結果です。図を見ると、その数が指数関数的に増えていることがわかります。2010年から2015年の5年間でおよそ2倍、2018年までなら推定で8倍に増えています。</p>\n",
    "\n",
    "<p><a href=\"https://camo.qiitausercontent.com/67cd328cd8d7a70f54a0f057703fd3b7377c8c12/687474703a2f2f7777772e64617461766572736974792e6e65742f77702d636f6e74656e742f75706c6f6164732f323031352f31302f64732e706e673f783338343032\" target=\"_blank\" rel=\"nofollow noopener\"><img src=\"https://camo.qiitausercontent.com/67cd328cd8d7a70f54a0f057703fd3b7377c8c12/687474703a2f2f7777772e64617461766572736974792e6e65742f77702d636f6e74656e742f75706c6f6164732f323031352f31302f64732e706e673f783338343032\" alt=\"データサイエンティストの増加\" data-canonical-src=\"http://www.dataversity.net/wp-content/uploads/2015/10/ds.png?x38402\"></a><br>\n",
    "出典: <a href=\"http://www.dataversity.net/study-shows-that-the-number-of-data-scientists-has-doubled-in-4-years/\" rel=\"nofollow noopener\" target=\"_blank\">Study Shows That the Number of Data Scientists Has Doubled in 4 Years</a></p>\n",
    "\n",
    "<p>データサイエンティストの数が急激に増えたことで、高度な分析をできる人手が足りていないという現状があります。そのような人手不足を補うために、データ分析や機械学習の適応経験が少ないエンジニアを助けるようなツールが必要となってきました。その目的に合致するのがAutoMLというわけです。</p>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<p>ここまでで、AutoMLの重要性が増している理由について述べました。次節からは各プロセスで従来行われている処理とAutoMLによる効率化について見ていきましょう。</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "<span id=\"automlで行われること\" class=\"fragment\"></span><a href=\"#automl%E3%81%A7%E8%A1%8C%E3%82%8F%E3%82%8C%E3%82%8B%E3%81%93%E3%81%A8\"><i class=\"fa fa-link\"></i></a>AutoMLで行われること</h1>\n",
    "\n",
    "<p>本節では機械学習の各プロセスでAutoMLがどのように生産性向上に寄与するかを説明します。対象とするプロセスは、ハイパーパラメータチューニング、モデル選択、特徴エンジニアリングの3つです。各プロセスについて、何をするプロセスなのか、なぜその処理が必要か、従来どうしていたか、AutoMLではどうしているかの3点から説明します。</p>\n",
    "\n",
    "<p><a href=\"https://camo.qiitausercontent.com/1ad7f4bde00d06fb81b35c74c08e92f60041cdb4/68747470733a2f2f64326d787565667165616137736a2e636c6f756466726f6e742e6e65742f735f343733314245454630393438373731374431443442453546324546354538443330413733333743363843303739303038444345423736384631413635344434375f313534313339323834393938315f2b323031382d31312d30352b31332e34302e31332e706e67\" target=\"_blank\" rel=\"nofollow noopener\"><img src=\"https://camo.qiitausercontent.com/1ad7f4bde00d06fb81b35c74c08e92f60041cdb4/68747470733a2f2f64326d787565667165616137736a2e636c6f756466726f6e742e6e65742f735f343733314245454630393438373731374431443442453546324546354538443330413733333743363843303739303038444345423736384631413635344434375f313534313339323834393938315f2b323031382d31312d30352b31332e34302e31332e706e67\" alt=\"出典: https://arxiv.org/pdf/1603.06212.pdf\" data-canonical-src=\"https://d2mxuefqeaa7sj.cloudfront.net/s_4731BEEF09487717D1D4BE5F2EF5E8D30A7337C68C079008DCEB768F1A654D47_1541392849981_+2018-11-05+13.40.13.png\"></a><br>\n",
    "出典: <a href=\"https://arxiv.org/abs/1603.06212\" rel=\"nofollow noopener\" target=\"_blank\">Evaluation of a Tree-based Pipeline Optimization Tool for Automating Data Science</a></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "<span id=\"ハイパーパラメータチューニング\" class=\"fragment\"></span><a href=\"#%E3%83%8F%E3%82%A4%E3%83%91%E3%83%BC%E3%83%91%E3%83%A9%E3%83%A1%E3%83%BC%E3%82%BF%E3%83%81%E3%83%A5%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0\"><i class=\"fa fa-link\"></i></a>ハイパーパラメータチューニング</h2>\n",
    "\n",
    "<p>ハイパーパラメータチューニングは、ハイパーパラメータを最適な値に調整するプロセスです。各機械学習モデルには様々なハイパーパラメータが存在します。たとえば、ランダムフォレストなら木の深さや数をハイパーパラメータとして持っています。これらのハイパーパラメータはデータから学習するものではなく、モデルを学習させる前に設定しておく必要があります。</p>\n",
    "\n",
    "<p>ハイパーパラメータチューニングが必要な理由として、機械学習フレームワークのデフォルトのパラメータでは良い性能が出ないことが多いという点を挙げることができます。以下の図は、scikit-learn に含まれる様々な機械学習アルゴリズムについて、ハイパーパラメータをデフォルト値からチューニングしたときに、性能がどれだけ向上したかを表しています。</p>\n",
    "\n",
    "<p><a href=\"https://camo.qiitausercontent.com/3a1652811ffde917d018519caaa46eabdb0c5f26/68747470733a2f2f64326d787565667165616137736a2e636c6f756466726f6e742e6e65742f735f343733314245454630393438373731374431443442453546324546354538443330413733333743363843303739303038444345423736384631413635344434375f313534313438313835353933325f2b323031382d31312d30362b31342e32332e33352e706e67\" target=\"_blank\" rel=\"nofollow noopener\"><img src=\"https://camo.qiitausercontent.com/3a1652811ffde917d018519caaa46eabdb0c5f26/68747470733a2f2f64326d787565667165616137736a2e636c6f756466726f6e742e6e65742f735f343733314245454630393438373731374431443442453546324546354538443330413733333743363843303739303038444345423736384631413635344434375f313534313438313835353933325f2b323031382d31312d30362b31342e32332e33352e706e67\" alt=\"ハイパーパラメータチューニングの効果\" data-canonical-src=\"https://d2mxuefqeaa7sj.cloudfront.net/s_4731BEEF09487717D1D4BE5F2EF5E8D30A7337C68C079008DCEB768F1A654D47_1541481855932_+2018-11-06+14.23.35.png\"></a><br>\n",
    "出典: <a href=\"https://arxiv.org/abs/1708.05070\" rel=\"nofollow noopener\" target=\"_blank\">Data-driven Advice for Applying Machine Learning to Bioinformatics Problems</a></p>\n",
    "\n",
    "<p>結果を見ると、アルゴリズムによって改善の度合いは異なりますが、ハイパーパラメータをチューニングすることで性能が向上することがわかります。平均的には正解率で3〜5%程度の改善が見られたという結果になっています。つまり、ハイパーパラメータは明らかにチューニングする価値があり、デフォルトのパラメータを信用し過ぎるべきではないということを示唆しています。</p>\n",
    "\n",
    "<p>チューニングによって性能向上が見込めるとはいえ、数多くのハイパーパラメータを手動でチューニングするのは骨が折れる作業です。たとえば、ハイパーパラメータ数が5個あり、各ハイパーパラメータに対して平均で3つの値をテストするのだとすれば、組合せは3の5乗（=273）通り存在します。これらすべての組み合わせに対して手動でチューニングをするのは非生産的な行為です。また、以下の図のようにモデルが複雑化すれば、手動でのチューニングは現実的ではなくなります。</p>\n",
    "\n",
    "<p><a href=\"https://camo.qiitausercontent.com/65c36e24413d87a17e13f63ba6f23f2676ac3739/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f37373037392f65646138393262352d326634612d396133642d326430352d3730613166313166353638312e706e67\" target=\"_blank\" rel=\"nofollow noopener\"><img src=\"https://camo.qiitausercontent.com/65c36e24413d87a17e13f63ba6f23f2676ac3739/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f37373037392f65646138393262352d326634612d396133642d326430352d3730613166313166353638312e706e67\" alt=\"Residual Network\" data-canonical-src=\"https://qiita-image-store.s3.amazonaws.com/0/77079/eda892b5-2f4a-9a3d-2d05-70a1f11f5681.png\"></a><br>\n",
    "出典: <a href=\"https://arxiv.org/abs/1512.03385\" rel=\"nofollow noopener\" target=\"_blank\">Deep Residual Learning for Image Recognition</a></p>\n",
    "\n",
    "<p>そこで、AutoMLでは従来人手で行っていたハイパーパラメータチューニングを自動化することを考えます。自動化により、チューニングの効率が向上するだけでなく、人が直感的に決めたパラメータによるバイアスを取り除くことにも繋がります。具体的には以下のような手法が使われています。</p>\n",
    "\n",
    "<ul>\n",
    "<li>グリッドサーチ(GridSearch)</li>\n",
    "<li>ランダムサーチ(RandomSearch)</li>\n",
    "<li>ベイズ最適化(Bayesian Optimization)</li>\n",
    "</ul>\n",
    "\n",
    "<p>このうち、最もよく使われているのはグリッドサーチとランダムサーチでしょう。それらの違いは以下の図のように表されます。</p>\n",
    "\n",
    "<p><a href=\"https://camo.qiitausercontent.com/08fa199b430570c45305439c1e7dd8df303aa3f4/68747470733a2f2f64326d787565667165616137736a2e636c6f756466726f6e742e6e65742f735f344439343233424443443239393133313146394144334644364235303431444637343945383641383135324539434432393939393434413941343141414146465f313534323638373335383234375f2b323031382d31312d32302b31332e31352e31362e706e67\" target=\"_blank\" rel=\"nofollow noopener\"><img src=\"https://camo.qiitausercontent.com/08fa199b430570c45305439c1e7dd8df303aa3f4/68747470733a2f2f64326d787565667165616137736a2e636c6f756466726f6e742e6e65742f735f344439343233424443443239393133313146394144334644364235303431444637343945383641383135324539434432393939393434413941343141414146465f313534323638373335383234375f2b323031382d31312d32302b31332e31352e31362e706e67\" alt=\"グリッドサーチとランダムサーチの違い\" data-canonical-src=\"https://d2mxuefqeaa7sj.cloudfront.net/s_4D9423BDCD2991311F9AD3FD6B5041DF749E86A8152E9CD2999944A9A41AAAFF_1542687358247_+2018-11-20+13.15.16.png\"></a><br>\n",
    "出典: <a href=\"http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf\" rel=\"nofollow noopener\" target=\"_blank\">Random Search for Hyper-Parameter Optimization</a></p>\n",
    "\n",
    "<p>グリッドサーチは、伝統的によく使われているハイパーパラメータチューニングの手法で、あらかじめ各ハイパーパラメータの候補値を複数設定して、すべての組合せを試すことでチューニングします。たとえば、$C$ と $\\gamma$ という2つのパラメータがあり、それぞれ、$C \\in$ {10, 100, 1000}, $\\gamma \\in$ {0.1, 0.2, 0.5, 1.0}という候補値を設定した場合、3x4=12の組み合わせについて試します。</p>\n",
    "\n",
    "\n",
    "\n",
    "<p>一方、ランダムサーチはパラメータに対する分布を指定し、そこから値をサンプリングしてチューニングする手法です。たとえば、グリッドサーチでは $C$ に対して $C \\in$ {10, 100, 1000} のような離散値を与えていたのに対して、ランダムサーチでは、パラメータ $\\lambda=100$ の指数分布のような確率分布を与え、そこから値をサンプリングします。少数のハイパーパラメータが性能に大きく影響を与える場合に効果的な手法です。</p>\n",
    "\n",
    "<p>グリッドサーチやランダムサーチの課題として、見込みのないハイパーパラメータに時間を費やしがちな点を挙げることができます。この原因としては、グリッドサーチやランダムサーチでは以前に得られた結果を利用していない点を挙げられます。</p>\n",
    "\n",
    "<p>では、以前に得られた結果を利用すると、どのようにハイパーパラメータを選べるのでしょうか？ 以下の図を御覧ください。この図はランダムフォレストのハイパーパラメータの一つである<code>n_estimator</code>の数を変化させたときの性能を示しています。スコアの値が高い方が性能が良いのだとすれば、<code>n_estimator</code>が200近辺を探索するより、800近辺を探索した方が効率が良さそうな事がわかると思います。</p>\n",
    "\n",
    "<p><a href=\"https://camo.qiitausercontent.com/d55f47c754062c04a3bdcb19db342b786179134c/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f37373037392f65383032633737382d373366382d323435632d636633652d3537356465333764326263612e706e67\" target=\"_blank\" rel=\"nofollow noopener\"><img src=\"https://camo.qiitausercontent.com/d55f47c754062c04a3bdcb19db342b786179134c/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f37373037392f65383032633737382d373366382d323435632d636633652d3537356465333764326263612e706e67\" alt=\"image.png\" data-canonical-src=\"https://qiita-image-store.s3.amazonaws.com/0/77079/e802c778-73f8-245c-cf3e-575de37d2bca.png\"></a><br>\n",
    "出典: <a href=\"https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f\" rel=\"nofollow noopener\" target=\"_blank\">A Conceptual Explanation of Bayesian Hyperparameter Optimization for Machine Learning</a></p>\n",
    "\n",
    "<p>最近使われるようになってきたベイズ最適化を用いたハイパーパラメータチューニングは、以前の結果を使って次に探索するハイパーパラメータを選ぶ手法です。これにより、有望そうなところを中心にハイパーパラメータを探索することができます。人間が行う探索に近いことをしているとも言えるでしょう。ディープラーニングを含む機械学習のモデルに対して、比較的良いハイパーパラメータを探索できることが知られています。</p>\n",
    "\n",
    "<p>ベイズ最適化の仕組みについてこれ以上詳しく解説するとこの記事では終わらないので、最後に最適化に使えるソフトウェアを紹介しましょう。ここでは以下のソフトウェアを挙げました。</p>\n",
    "\n",
    "<ul>\n",
    "<li><a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\" rel=\"nofollow noopener\" target=\"_blank\">GridSearchCV</a></li>\n",
    "<li><a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html\" rel=\"nofollow noopener\" target=\"_blank\">RandomizedSearchCV</a></li>\n",
    "<li><a href=\"https://github.com/hyperopt/hyperopt\" rel=\"nofollow noopener\" target=\"_blank\">hyperopt</a></li>\n",
    "<li><a href=\"https://github.com/maxpumperla/hyperas\" rel=\"nofollow noopener\" target=\"_blank\">hyperas</a></li>\n",
    "</ul>\n",
    "\n",
    "<p><a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\" rel=\"nofollow noopener\" target=\"_blank\">GridSearchCV</a>と<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html\" rel=\"nofollow noopener\" target=\"_blank\">RandomizedSearchCV</a>はご存知 scikit-learn に組み込まれているクラスです。scikit-learn を使っている場合には一番使いやすいのではないかと思います。 scikit-learn に限らず使いたいなら、<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ParameterGrid.html\" rel=\"nofollow noopener\" target=\"_blank\">ParameterGrid</a> を使うのが選択肢に挙がると思います。ParameterGrid を使うことでハイパーパラメータの組み合わせを生成することができます。</p>\n",
    "\n",
    "<p><a href=\"https://github.com/hyperopt/hyperopt\" rel=\"nofollow noopener\" target=\"_blank\">hyperopt</a> はランダムサーチとベイズ最適化によるハイパーパラメータチューニングを行えるPythonパッケージです。ベイズ最適化の方はTPEと呼ばれるアルゴリズムをサポートしています。<a href=\"https://github.com/maxpumperla/hyperas\" rel=\"nofollow noopener\" target=\"_blank\">hyperas</a> はKeras用のhyperoptラッパーです。私のようにKerasをよく使うユーザにはこちらの方が使いやすいと思います。</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "<span id=\"モデル選択\" class=\"fragment\"></span><a href=\"#%E3%83%A2%E3%83%87%E3%83%AB%E9%81%B8%E6%8A%9E\"><i class=\"fa fa-link\"></i></a>モデル選択</h2>\n",
    "\n",
    "<p>モデル選択は、データを学習させるのに使う機械学習アルゴリズムを選ぶプロセスです。モデルにはSVMやランダムフォレスト、ニューラルネットワークなど多くの種類があります。その中から、解きたい問題に応じて選びます。たとえば、解釈性が重要な場合は決定木などのモデルが選ばれるでしょうし、とにかく性能を出したいという場合はニューラルネットワークが候補になるでしょう。</p>\n",
    "\n",
    "<p>モデル選択が必要な理由として、すべての問題に最適な機械学習アルゴリズムは存在しないという点を挙げることができます。以下の図は165個のデータセットについて、モデル間の性能の勝敗について検証した結果を示しています。左側の列はモデルの名前が書かれており、良い結果となったモデルから順に並んでいます。</p>\n",
    "\n",
    "<p><a href=\"https://camo.qiitausercontent.com/f04b3e0c2c07c165f03fc75846e09608461b7322/68747470733a2f2f64326d787565667165616137736a2e636c6f756466726f6e742e6e65742f735f343733314245454630393438373731374431443442453546324546354538443330413733333743363843303739303038444345423736384631413635344434375f313534313438313933333736365f2b323031382d31312d30362b31342e32352e31342e706e67\" target=\"_blank\" rel=\"nofollow noopener\"><img src=\"https://camo.qiitausercontent.com/f04b3e0c2c07c165f03fc75846e09608461b7322/68747470733a2f2f64326d787565667165616137736a2e636c6f756466726f6e742e6e65742f735f343733314245454630393438373731374431443442453546324546354538443330413733333743363843303739303038444345423736384631413635344434375f313534313438313933333736365f2b323031382d31312d30362b31342e32352e31342e706e67\" alt=\"出典: https://arxiv.org/pdf/1708.05070.pdf\" data-canonical-src=\"https://d2mxuefqeaa7sj.cloudfront.net/s_4731BEEF09487717D1D4BE5F2EF5E8D30A7337C68C079008DCEB768F1A654D47_1541481933766_+2018-11-06+14.25.14.png\"></a><br>\n",
    "出典: <a href=\"https://arxiv.org/abs/1708.05070\" rel=\"nofollow noopener\" target=\"_blank\">Data-driven Advice for Applying Machine Learning to Bioinformatics Problems</a></p>\n",
    "\n",
    "<p>結果を見ると、Gradient Tree Boosting（GTB）やランダムフォレスト、SVMは良く、逆にNBは悪いことがわかります。また、ほとんどの場合においてGradient Tree Boostingは良い結果なのですが、NBでも1%のデータセットではGTBに勝っているという結果になっています。ちなみに、足しても100%にならないのは、少なくとも正解率で1%以上上回った場合を勝利としているからです。</p>\n",
    "\n",
    "<p>要するに何が言いたいかというと、確かにGTBやRandomForestは良い結果を出しますが、すべての問題で勝てる最適なアルゴリズムは存在しないということです。これは機械学習を行う上で重要な点で、機械学習を使って問題を解く際には、多くの機械学習アルゴリズムについて考慮する必要があるということをこの実験結果は示唆しています。</p>\n",
    "\n",
    "<p>ただ、実際のプロジェクトでは多くの機械学習アルゴリズムを考慮できているとはいい難い状況です。その原因の一つには、人間のバイアスが関係しています。たとえば、「GTBは毎回良い結果を出すからこれを使っておけばいいんだ」というのは一つのバイアスです。確かにそれはたいていの場合正しいかもしれません。しかし、上図が示すように、実際には人間のバイアスは悪い方向に働くこともあるのです。</p>\n",
    "\n",
    "<p>人間のバイアスを軽減させるために有効な手の一つとして、データセットの特徴に応じて選ぶモデルを決定する仕組みを構築しておく手があります。以下は scikit-learn が公開している機械学習アルゴリズムを選択するためのチートシートです。ただ、この方法にもシートを作成した人のバイアスが入っている、多くのアルゴリズムを考慮できていないといった問題があります。</p>\n",
    "\n",
    "<p><a href=\"https://camo.qiitausercontent.com/4430afeff30fd087e781a43c9b98ca6fef9b7f5e/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f37373037392f61303861623334352d313432372d326566392d313535382d6235373664383233643365612e706e67\" target=\"_blank\" rel=\"nofollow noopener\"><img src=\"https://camo.qiitausercontent.com/4430afeff30fd087e781a43c9b98ca6fef9b7f5e/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f37373037392f61303861623334352d313432372d326566392d313535382d6235373664383233643365612e706e67\" alt=\"model_selection.png\" data-canonical-src=\"https://qiita-image-store.s3.amazonaws.com/0/77079/a08ab345-1427-2ef9-1558-b576d823d3ea.png\"></a><br>\n",
    "出典: <a href=\"https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html\" rel=\"nofollow noopener\" target=\"_blank\">Choosing the right estimator</a></p>\n",
    "\n",
    "<p>そういうわけでAutoMLでは機械学習アルゴリズムの選択を自動的に行うことを考えます。モデル選択を自動化することにより、人間のバイアスを排除しつつ、様々なモデルを考慮することができます。モデル選択についてはハイパーパラメータチューニングと切り離せない話なので、話としてはここまでにしておきます。</p>\n",
    "\n",
    "<p>お話だけだと退屈なので、ここでソフトウェアを紹介しましょう。モデル選択の機能を組み込んだソフトウェアは商用・非商用問わずに数多くありますが、今回はその中から <a href=\"https://github.com/EpistasisLab/tpot\" rel=\"nofollow noopener\" target=\"_blank\">TPOT</a> を紹介します。</p>\n",
    "\n",
    "<p><a href=\"https://github.com/EpistasisLab/tpot\" rel=\"nofollow noopener\" target=\"_blank\">TPOT</a> は、scikit-learnライクなAPIで使えるAutoMLのツールです。機能としてはモデル選択とハイパーパラメータチューニングを行ってくれます。また、タスクとしては分類と回帰を解くことができます。</p>\n",
    "\n",
    "<p>scikit-learnを使ったことがある人であれば、TPOTを使うのは非常に簡単です。たとえば、分類問題を解く場合は、<code>TPOTClassifier</code>をインポートし、データを与えて学習させるだけです。以下ではMNISTを学習させています。</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1 - Current best internal CV score: 0.9598211199357554\n",
      "Generation 2 - Current best internal CV score: 0.973236702534767\n",
      "Generation 3 - Current best internal CV score: 0.9798926351106825\n",
      "Generation 4 - Current best internal CV score: 0.9798926351106825\n",
      "Generation 5 - Current best internal CV score: 0.9798926351106825\n",
      "\n",
      "Best pipeline: KNeighborsClassifier(input_matrix, n_neighbors=2, p=2, weights=uniform)\n",
      "0.991111111111\n"
     ]
    }
   ],
   "source": [
    "from tpot import TPOTClassifier\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "digits = load_digits()\n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target,\n",
    "                                                    train_size=0.75, test_size=0.25)\n",
    "\n",
    "tpot = TPOTClassifier(generations=5, population_size=20, verbosity=2)\n",
    "tpot.fit(X_train, y_train)\n",
    "print(tpot.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>学習が終わると最も良かったパイプラインとそのスコアを表示します。今回の場合、モデルに<code>KNeighborsClassifier</code>を使うのが最も良い結果になりました。ハイパーパラメータが設定されていることも確認できます。スコア自体はよくありませんでしたが、手軽さは確認できたかと思います。</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>\n",
    "<span id=\"ニューラルアーキテクチャサーチ\" class=\"fragment\"></span><a href=\"#%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E3%82%A2%E3%83%BC%E3%82%AD%E3%83%86%E3%82%AF%E3%83%81%E3%83%A3%E3%82%B5%E3%83%BC%E3%83%81\"><i class=\"fa fa-link\"></i></a>ニューラルアーキテクチャサーチ</h3>\n",
    "\n",
    "<p>モデル選択と関係する話として、最近よく話題になるニューラルアーキテクチャサーチ（Neural Architecture Search: NAS）について述べておきましょう。NASもAutoMLの一部と捉えられます。ニュースでも大きく取り上げられ、New York Timesでは「AIを構築できるAIを構築する」（<a href=\"https://www.nytimes.com/2017/11/05/technology/machine-learning-artificial-intelligence-ai.html\" rel=\"nofollow noopener\" target=\"_blank\">Building A.I. That Can Build A.I.</a>）というタイトルで記事が書かれています。</p>\n",
    "\n",
    "<p><a href=\"https://camo.qiitausercontent.com/7fd93580a38bd29d9f9fe2ad582a585f26f28636/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f37373037392f39323735393462642d306639312d336436632d303739332d6332633539626532336536332e706e67\" target=\"_blank\" rel=\"nofollow noopener\"><img width=\"640\" alt=\"スクリーンショット 2018-12-03 9.56.03.png\" src=\"https://camo.qiitausercontent.com/7fd93580a38bd29d9f9fe2ad582a585f26f28636/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f37373037392f39323735393462642d306639312d336436632d303739332d6332633539626532336536332e706e67\" data-canonical-src=\"https://qiita-image-store.s3.amazonaws.com/0/77079/927594bd-0f91-3d6c-0793-c2c59be23e63.png\"></a></p>\n",
    "\n",
    "\n",
    "\n",
    "<p>ニューラルアーキテクチャサーチとは、ニューラルネットワークの構造設計を自動化する技術です。実際には、ニューラルネットワークを使ってネットワークアーキテクチャを生成し、ハイパーパラメータチューニングをしつつ学習させています。</p>\n",
    "\n",
    "\n",
    "\n",
    "<p>基本的な枠組みは以下の図のようになっています。まず、コントローラと呼ばれるRNNがアーキテクチャをサンプリングします。次に、サンプリングした結果を使って、ネットワークを構築します。そして、構築したネットワークを学習し、検証用データセットに対して評価を行います。この評価結果を使って、より良いアーキテクチャを設計できるようにコントローラを更新します。以上の操作を繰り返し行うことで良いアーキテクチャを探索しています。</p>\n",
    "\n",
    "<p><a href=\"https://camo.qiitausercontent.com/f22cb2bb3b7b06b0fdf3797f144921b0275c5285/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f37373037392f38343664323863342d663032332d393863362d653964612d6162353830616432306261642e706e67\" target=\"_blank\" rel=\"nofollow noopener\"><img width=\"893\" alt=\"スクリーンショット 2018-12-03 10.14.50.png\" src=\"https://camo.qiitausercontent.com/f22cb2bb3b7b06b0fdf3797f144921b0275c5285/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f37373037392f38343664323863342d663032332d393863362d653964612d6162353830616432306261642e706e67\" data-canonical-src=\"https://qiita-image-store.s3.amazonaws.com/0/77079/846d28c4-f023-98c6-e9da-ab580ad20bad.png\"></a><br>\n",
    "出典: <a href=\"http://rll.berkeley.edu/deeprlcoursesp17/docs/quoc_barret.pdf\" rel=\"nofollow noopener\" target=\"_blank\">Neural Architecture Search with Reinforcement Learning</a></p>\n",
    "\n",
    "\n",
    "\n",
    "<p>ニューラルネットワークの設計を自動化したいのにはいくつかの理由があります。その一つとしてニューラルネットワークのアーキテクチャを設計するのは高度な専門知識が必要で非常に難しい点を挙げられます。よいアーキテクチャを作るためには試行錯誤が必要で、これには時間もお金もかかります。これでは活用できるのが少数の研究者やエンジニアだけに限られてしまいます。</p>\n",
    "\n",
    "\n",
    "\n",
    "<p>このような理由からニューラルアーキテクチャサーチで設計から学習まで自動化しようという話になりました。NASによって、アーキテクチャの設計、ハイパーパラメータチューニング、学習を自動化することができ、誰にでも利用できるようになります。これはつまり、ドメインエキスパートによるニューラルネットワークの活用に道が開かれることを意味しています。</p>\n",
    "\n",
    "\n",
    "\n",
    "<p>そんなNASの課題としては計算量の多さを挙げられます。たとえば、<a href=\"http://rll.berkeley.edu/deeprlcoursesp17/docs/quoc_barret.pdf\" rel=\"nofollow noopener\" target=\"_blank\">Neural Architecture Search with Reinforcement Learning</a>では、アーキテクチャを探索するのに 800 GPUで28日間かかっています。また、<a href=\"https://arxiv.org/abs/1707.07012\" rel=\"nofollow noopener\" target=\"_blank\">NASNet</a>では、500 GPU を使用して4日間かかっています。これでは一般の研究者や開発者が利用するのは現実的ではありません。</p>\n",
    "\n",
    "\n",
    "\n",
    "<p>高速化の手段の一つとして使われるのが転移学習です。<a href=\"https://arxiv.org/abs/1802.03268\" rel=\"nofollow noopener\" target=\"_blank\">Efficient Neural Architecture Search via Parameter Sharing</a>ではすべての重みをスクラッチで学習させるのではなく、学習済みのモデルから転移学習させて使うことで高速化をしています。その結果、学習時間は 1 GPU で半日までに抑えられています。</p>\n",
    "\n",
    "<p>最後にNASを提供しているサービスとOSSについて紹介します。</p>\n",
    "\n",
    "<p>NASを提供するサービスとして最も有名なのはGoogleの <a href=\"https://cloud.google.com/automl/\" rel=\"nofollow noopener\" target=\"_blank\">Cloud AutoML</a> でしょう。Cloud AutoMLでは画像認識、テキスト分類、翻訳に関して学習させることができます。データさえ用意すれば、誰でも簡単に良いモデルを作って使えるのが特徴です。一方、お金が結構かかるのと、学習したモデルをエクスポートできないのが欠点です。</p>\n",
    "\n",
    "<p>NASに使えるOSSとしてはAuto-Kerasがあります。Auto-Kerasは、scikit-learnライクなAPIで使えるオープンソースのAutoMLのツールです。こちらは、Texas A&amp;M大学のDATA Labとコミュニティによって開発されました。論文としては、2018年に発表された「Auto-Keras: Efficient Neural Architecture Search with Network Morphism」が基となっています。目標としては、機械学習の知識のないドメインエキスパートでも簡単にディープラーニングを使えるようにすることです。</p>\n",
    "\n",
    "<p>Auto-Kerasもscikit-learnを触ったことがある人であれば使うのは簡単です。以下のようにして分類器を定義し、<code>fit</code>メソッドを使って学習させるだけです。以下のコードはAuto-KerasにMNISTを学習させるコードです。<code>fit</code>メソッドで学習をして最適なアーキテクチャを探索します。その後、<code>final_fit</code>で探索を終えて得られた最適なアーキテクチャで学習し直します。</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"code-frame\" data-lang=\"python\"><div class=\"highlight\"><pre><span class=\"kn\">import</span> <span class=\"nn\">autokeras</span> <span class=\"k\">as</span> <span class=\"n\">ak</span>\n",
    "<span class=\"kn\">from</span> <span class=\"nn\">keras.datasets</span> <span class=\"kn\">import</span> <span class=\"n\">mnist</span>\n",
    "\n",
    "\n",
    "<span class=\"k\">if</span> <span class=\"n\">__name__</span> <span class=\"o\">==</span> <span class=\"s\">'__main__'</span><span class=\"p\">:</span>\n",
    "    <span class=\"p\">(</span><span class=\"n\">x_train</span><span class=\"p\">,</span> <span class=\"n\">y_train</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"n\">x_test</span><span class=\"p\">,</span> <span class=\"n\">y_test</span><span class=\"p\">)</span> <span class=\"o\">=</span> <span class=\"n\">mnist</span><span class=\"o\">.</span><span class=\"n\">load_data</span><span class=\"p\">()</span>\n",
    "    <span class=\"n\">x_train</span> <span class=\"o\">=</span> <span class=\"n\">x_train</span><span class=\"o\">.</span><span class=\"n\">reshape</span><span class=\"p\">(</span><span class=\"n\">x_train</span><span class=\"o\">.</span><span class=\"n\">shape</span> <span class=\"o\">+</span> <span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,))</span>\n",
    "    <span class=\"n\">x_test</span> <span class=\"o\">=</span> <span class=\"n\">x_test</span><span class=\"o\">.</span><span class=\"n\">reshape</span><span class=\"p\">(</span><span class=\"n\">x_test</span><span class=\"o\">.</span><span class=\"n\">shape</span> <span class=\"o\">+</span> <span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,))</span>\n",
    "\n",
    "    <span class=\"n\">clf</span> <span class=\"o\">=</span> <span class=\"n\">ak</span><span class=\"o\">.</span><span class=\"n\">ImageClassifier</span><span class=\"p\">(</span><span class=\"n\">verbose</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)</span>\n",
    "    <span class=\"n\">clf</span><span class=\"o\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">x_train</span><span class=\"p\">,</span> <span class=\"n\">y_train</span><span class=\"p\">,</span> <span class=\"n\">time_limit</span><span class=\"o\">=</span><span class=\"mi\">12</span> <span class=\"o\">*</span> <span class=\"mi\">60</span> <span class=\"o\">*</span> <span class=\"mi\">60</span><span class=\"p\">)</span>\n",
    "    <span class=\"n\">clf</span><span class=\"o\">.</span><span class=\"n\">final_fit</span><span class=\"p\">(</span><span class=\"n\">x_train</span><span class=\"p\">,</span> <span class=\"n\">y_train</span><span class=\"p\">,</span> <span class=\"n\">x_test</span><span class=\"p\">,</span> <span class=\"n\">y_test</span><span class=\"p\">,</span> <span class=\"n\">retrain</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)</span>\n",
    "    <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">clf</span><span class=\"o\">.</span><span class=\"n\">evaluate</span><span class=\"p\">(</span><span class=\"n\">x_test</span><span class=\"p\">,</span> <span class=\"n\">y_test</span><span class=\"p\">))</span>\n",
    "</pre></div></div>\n",
    "\n",
    "<p>学習を始めると以下のような表示がされます。<code>Father Model</code>に親モデルのIDが書いてあり、その隣に変更点が書いてあります。そして、学習が終わるとその結果が<code>Loss</code>と<code>Metric Value</code>に表示されます。このモデルの場合は、正解率で<code>0.9952</code>だったことを表しています。</p>\n",
    "\n",
    "<div class=\"code-frame\" data-lang=\"python\"><div class=\"highlight\"><pre><span class=\"o\">+----------------------------------------------+</span>\n",
    "<span class=\"o\">|</span>               <span class=\"n\">Training</span> <span class=\"n\">model</span> <span class=\"mi\">8</span>               <span class=\"o\">|</span>\n",
    "<span class=\"o\">+----------------------------------------------+</span>\n",
    "\n",
    "<span class=\"o\">+--------------------------------------------------------------------------+</span>\n",
    "<span class=\"o\">|</span>    <span class=\"n\">Father</span> <span class=\"n\">Model</span> <span class=\"n\">ID</span>     <span class=\"o\">|</span>                 <span class=\"n\">Added</span> <span class=\"n\">Operation</span>                 <span class=\"o\">|</span>\n",
    "<span class=\"o\">+--------------------------------------------------------------------------+</span>\n",
    "<span class=\"o\">|</span>           <span class=\"mi\">6</span>            <span class=\"o\">|</span>    <span class=\"n\">to_deeper_model</span> <span class=\"mi\">68</span> <span class=\"n\">Conv2d</span><span class=\"p\">(</span><span class=\"mi\">512</span><span class=\"p\">,</span> <span class=\"mi\">512</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">)</span>    <span class=\"o\">|</span>\n",
    "<span class=\"o\">+--------------------------------------------------------------------------+</span>\n",
    "<span class=\"n\">Saving</span> <span class=\"n\">model</span><span class=\"o\">.</span>\n",
    "<span class=\"o\">+--------------------------------------------------------------------------+</span>\n",
    "<span class=\"o\">|</span>        <span class=\"n\">Model</span> <span class=\"n\">ID</span>        <span class=\"o\">|</span>          <span class=\"n\">Loss</span>          <span class=\"o\">|</span>      <span class=\"n\">Metric</span> <span class=\"n\">Value</span>      <span class=\"o\">|</span>\n",
    "<span class=\"o\">+--------------------------------------------------------------------------+</span>\n",
    "<span class=\"o\">|</span>           <span class=\"mi\">8</span>            <span class=\"o\">|</span>  <span class=\"mf\">0.07076152432709933</span>   <span class=\"o\">|</span>         <span class=\"mf\">0.9952</span>         <span class=\"o\">|</span>\n",
    "<span class=\"o\">+--------------------------------------------------------------------------+</span>\n",
    "</pre></div></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "<span id=\"特徴エンジニアリング\" class=\"fragment\"></span><a href=\"#%E7%89%B9%E5%BE%B4%E3%82%A8%E3%83%B3%E3%82%B8%E3%83%8B%E3%82%A2%E3%83%AA%E3%83%B3%E3%82%B0\"><i class=\"fa fa-link\"></i></a>特徴エンジニアリング</h2>\n",
    "\n",
    "\n",
    "\n",
    "<p>特徴エンジニアリング（Feature Engineering）は、機械学習アルゴリズムがうまく学習できるような特徴を作成するプロセスのことです。特徴エンジニアリングは機械学習を使ったシステムを作る際の基礎であり、ここで良い特徴を得られれば、機械学習アルゴリズムでも良い性能を出すことができます。ただし、良い特徴を得るのは非常に難しく、時間のかかる部分でもあります。</p>\n",
    "\n",
    "\n",
    "\n",
    "<p>機械学習における特徴（Feature）とは、観察される現象の測定可能な特性のことです。以下の例は、タイタニック号の生存者情報を含むデータセットの一部です。各列は、分析に使用できる測定可能なデータである年齢（Age）、性別（Sex）、料金（Fare）などを含みます。これらがデータセットの特徴量です。</p>\n",
    "\n",
    "<p><a href=\"https://camo.qiitausercontent.com/8e7a03aef92938afd5eded43ff0e8e504921de2b/68747470733a2f2f64326d787565667165616137736a2e636c6f756466726f6e742e6e65742f735f344439343233424443443239393133313146394144334644364235303431444637343945383641383135324539434432393939393434413941343141414146465f313534323630343733363438315f746974616e69632e706e67\" target=\"_blank\" rel=\"nofollow noopener\"><img src=\"https://camo.qiitausercontent.com/8e7a03aef92938afd5eded43ff0e8e504921de2b/68747470733a2f2f64326d787565667165616137736a2e636c6f756466726f6e742e6e65742f735f344439343233424443443239393133313146394144334644364235303431444637343945383641383135324539434432393939393434413941343141414146465f313534323630343733363438315f746974616e69632e706e67\" alt=\"タイタニック号の生存者情報を含むデータセット\" data-canonical-src=\"https://d2mxuefqeaa7sj.cloudfront.net/s_4D9423BDCD2991311F9AD3FD6B5041DF749E86A8152E9CD2999944A9A41AAAFF_1542604736481_titanic.png\"></a></p>\n",
    "\n",
    "\n",
    "\n",
    "<p>特徴エンジニアリングが重要な理由として、良い特徴を得ることで機械学習アルゴリズムの予測性能が大きく変わる点を挙げることができます。たとえば、タイタニック号のデータセットの例で言うなら、乗客名をそのまま特徴として使っても良い性能は得られないでしょう。しかし、名前から敬称（Mr. Mrs. Sir.など）を抽出して使えば性能向上に役立つでしょう。なぜなら、社会的地位の高さや既婚者か否かといった情報は救命ボートに乗る際に考慮されただろうと考えられるからです。</p>\n",
    "\n",
    "\n",
    "\n",
    "<p>伝統的に、特徴エンジニアリングは人間によって行われてきましたが、それには2つの問題点がありました。一つは良い特徴を思いつくのは難しいという点です。良い特徴を思いつくためにはドメインの知識が必要な場合もあり、一筋縄ではいきません。もう一つは、時間がかかるという点です。単に思いつくだけではなく、それを検証することも含めると、特徴エンジニアリングは非常に時間のかかる作業です。実際に機械学習のどの部分で時間がかかるかをデータサイエンティストに尋ねると特徴エンジニアリングは上位に位置します。</p>\n",
    "\n",
    "\n",
    "\n",
    "<p>AutoMLでは、人間によって行われてきた特徴エンジニアリングを自動化します。これにより、先に述べた2つの問題を軽減することができます。以下では実際にAutoMLにおいて特徴エンジニアリングがどのように行われるのかについて説明します。ここでは、AutoMLのサービスである <strong>DataRobot</strong> での自動化と、特徴エンジニアリングを自動化するためのツールである <strong>featuretools</strong> を例にとって説明しましょう。</p>\n",
    "\n",
    "\n",
    "\n",
    "<p>まずはAutoMLのサービスである <a href=\"https://www.datarobot.com/\" rel=\"nofollow noopener\" target=\"_blank\">DataRobot</a> ではどうしているかを紹介しましょう。以下の述べる内容は、DataRobotのブログ記事「<a href=\"https://blog.datarobot.com/automated-feature-engineering\" rel=\"nofollow noopener\" target=\"_blank\">Automated Feature Engineering</a>」に基づいています。2018年6月28日の記事ですが内容が古くなっている可能性はありますので、その点はご留意ください。</p>\n",
    "\n",
    "<p>DataRobotではエキスパートシステムを構築することで特徴エンジニアリングを自動化しています。具体的には以下のようなことをしています。</p>\n",
    "\n",
    "<ul>\n",
    "<li>特徴の生成</li>\n",
    "<li>特徴エンジニアリングが必要なモデルを知る</li>\n",
    "<li>各モデルに有効な特徴エンジニアリングの種類を知る</li>\n",
    "<li>システマティックにモデルを比較して、特徴エンジニアリングとモデルの最も良い組み合わせを知る</li>\n",
    "</ul>\n",
    "\n",
    "<p>これらの操作をDataRobotでは model blueprint を使って行っています。ここで、model blueprint とは、<a href=\"https://blog.datarobot.com/data/model-blueprints-add-value-datarobot\" rel=\"nofollow noopener\" target=\"_blank\">こちらの記事</a>によると、前処理、特徴エンジニアリング、学習、チューニングといった処理のシーケンスのことのようです。以下が model blueprint の例です。</p>\n",
    "\n",
    "<p><a href=\"https://camo.qiitausercontent.com/9ca394339e6d980262f46812711de4adf19d74e9/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f37373037392f36663634646131332d646438332d653839612d333033312d3334373464616664613635632e706e67\" target=\"_blank\" rel=\"nofollow noopener\"><img src=\"https://camo.qiitausercontent.com/9ca394339e6d980262f46812711de4adf19d74e9/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f37373037392f36663634646131332d646438332d653839612d333033312d3334373464616664613635632e706e67\" alt=\"image.png\" data-canonical-src=\"https://qiita-image-store.s3.amazonaws.com/0/77079/6f64da13-dd83-e89a-3031-3474dafda65c.png\"></a><br>\n",
    "出典: <a href=\"https://blog.datarobot.com/automated-feature-engineering\" rel=\"nofollow noopener\" target=\"_blank\">Automated Feature Engineering</a></p>\n",
    "\n",
    "<p>この model blueprint では、DataRobotのシステムはL2正則化を入れたロジスティック回帰に対するデータを用意しています。具体的に行っている特徴エンジニアリングとしては、One-Hotエンコーディング、欠損値の補完、標準化の3つです。</p>\n",
    "\n",
    "<p>より複雑な例としては以下の Gradient Boosted Greedy Treesに対する model blueprint があります。</p>\n",
    "\n",
    "<p><a href=\"https://camo.qiitausercontent.com/61466c27b608d79afd0a1a76bcde73dd080c39d8/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f37373037392f30343134393062382d656237652d353566382d626438622d3764333338393164383261622e706e67\" target=\"_blank\" rel=\"nofollow noopener\"><img src=\"https://camo.qiitausercontent.com/61466c27b608d79afd0a1a76bcde73dd080c39d8/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f37373037392f30343134393062382d656237652d353566382d626438622d3764333338393164383261622e706e67\" alt=\"image.png\" data-canonical-src=\"https://qiita-image-store.s3.amazonaws.com/0/77079/041490b8-eb7e-55f8-bd8b-7d33891d82ab.png\"></a><br>\n",
    "出典: <a href=\"https://blog.datarobot.com/automated-feature-engineering\" rel=\"nofollow noopener\" target=\"_blank\">Automated Feature Engineering</a></p>\n",
    "\n",
    "<p>これまでのところをまとめましょう。まず、DataRobotでは前処理からチューニングまでのパイプラインを定義したmodel blueprintが多数用意されていいます。それらのmodel blueprintをデータに対して適用し、その結果を比較して最も良いモデルと特徴エンジニアリングの組み合わせを決めているということをしているようです。</p>\n",
    "\n",
    "<p>DataRobotの方法も良いのですが、作り込みが必要で真似しにくい感じのやり方なので <a href=\"https://github.com/Featuretools/featuretools\" rel=\"nofollow noopener\" target=\"_blank\">featuretools</a> についても紹介しておきます。featuretools はPython製のオープンソースの特徴エンジニアリング自動化ツールです。featuretools を使うことで特徴を自動的に生成することができます。</p>\n",
    "\n",
    "<p>featuretools では Deep Feature Synthesis(DFS) と呼ばれる方法で新たな特徴を生成しています。DFSでは primitive と呼ばれる関数を使ってデータの集約と変換を行います。primitive の例としては、列の平均や最大値を取る関数を挙げることができます。また自分で定義した関数を primitive として使うこともできます。</p>\n",
    "\n",
    "<p>百聞は一見にしかずということで、実際にやってみましょう。まずはデモ用のカスタマートランザクションデータを生成します。</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import featuretools as ft\n",
    "import pprint\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Entityset: transactions\n",
       "  Entities:\n",
       "    customers [Rows: 5, Columns: 3]\n",
       "    products [Rows: 5, Columns: 2]\n",
       "    sessions [Rows: 35, Columns: 4]\n",
       "    transactions [Rows: 500, Columns: 5]\n",
       "  Relationships:\n",
       "    transactions.product_id -> products.product_id\n",
       "    transactions.session_id -> sessions.session_id\n",
       "    sessions.customer_id -> customers.customer_id"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es = ft.demo.load_mock_customer(return_entityset=True)\n",
    "es"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>生成されたデータには4つのテーブル(transactions, products, sessions, customers)と3つの関係が定義されています。このうち、<code>customers</code>テーブルを対象にprimitiveを適用して特徴を生成します。以下がそのコードです。</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zip_code</th>\n",
       "      <th>NUM_UNIQUE(sessions.device)</th>\n",
       "      <th>WEEKDAY(join_date)</th>\n",
       "      <th>YEAR(join_date)</th>\n",
       "      <th>MONTH(join_date)</th>\n",
       "      <th>DAY(join_date)</th>\n",
       "      <th>MODE(transactions.product_id)</th>\n",
       "      <th>MODE(sessions.device)</th>\n",
       "      <th>SUM(transactions.amount)</th>\n",
       "      <th>STD(transactions.amount)</th>\n",
       "      <th>...</th>\n",
       "      <th>NUM_UNIQUE(sessions.MONTH(session_start))</th>\n",
       "      <th>SUM(sessions.MIN(transactions.amount))</th>\n",
       "      <th>SKEW(sessions.STD(transactions.amount))</th>\n",
       "      <th>MAX(sessions.SUM(transactions.amount))</th>\n",
       "      <th>MEAN(sessions.MAX(transactions.amount))</th>\n",
       "      <th>STD(sessions.SUM(transactions.amount))</th>\n",
       "      <th>MEAN(sessions.STD(transactions.amount))</th>\n",
       "      <th>MIN(sessions.MAX(transactions.amount))</th>\n",
       "      <th>MODE(sessions.DAY(session_start))</th>\n",
       "      <th>SUM(sessions.MEAN(transactions.amount))</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>customer_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>60091</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2008</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>desktop</td>\n",
       "      <td>10236.77</td>\n",
       "      <td>42.837081</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>169.77</td>\n",
       "      <td>0.062233</td>\n",
       "      <td>1248.67</td>\n",
       "      <td>143.74700</td>\n",
       "      <td>185.456436</td>\n",
       "      <td>43.056690</td>\n",
       "      <td>131.83</td>\n",
       "      <td>1</td>\n",
       "      <td>791.976505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>02139</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>mobile</td>\n",
       "      <td>9118.81</td>\n",
       "      <td>43.382936</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>114.85</td>\n",
       "      <td>-0.121708</td>\n",
       "      <td>1461.66</td>\n",
       "      <td>139.85375</td>\n",
       "      <td>246.236871</td>\n",
       "      <td>42.876605</td>\n",
       "      <td>124.29</td>\n",
       "      <td>1</td>\n",
       "      <td>596.243506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>02139</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2008</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>desktop</td>\n",
       "      <td>5758.24</td>\n",
       "      <td>40.387654</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>64.98</td>\n",
       "      <td>-1.523536</td>\n",
       "      <td>2054.32</td>\n",
       "      <td>135.16400</td>\n",
       "      <td>526.648290</td>\n",
       "      <td>38.539577</td>\n",
       "      <td>111.02</td>\n",
       "      <td>1</td>\n",
       "      <td>369.770121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60091</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2008</td>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>desktop</td>\n",
       "      <td>8205.28</td>\n",
       "      <td>42.047038</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>83.53</td>\n",
       "      <td>-0.269755</td>\n",
       "      <td>1609.13</td>\n",
       "      <td>140.04500</td>\n",
       "      <td>345.176925</td>\n",
       "      <td>41.730640</td>\n",
       "      <td>108.05</td>\n",
       "      <td>1</td>\n",
       "      <td>584.673126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>02139</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2008</td>\n",
       "      <td>7</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>tablet</td>\n",
       "      <td>4571.37</td>\n",
       "      <td>43.028739</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>73.09</td>\n",
       "      <td>0.943485</td>\n",
       "      <td>1356.60</td>\n",
       "      <td>141.50000</td>\n",
       "      <td>229.234047</td>\n",
       "      <td>42.696139</td>\n",
       "      <td>132.72</td>\n",
       "      <td>1</td>\n",
       "      <td>313.448942</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 69 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            zip_code  NUM_UNIQUE(sessions.device)  WEEKDAY(join_date)  \\\n",
       "customer_id                                                             \n",
       "1              60091                            3                   1   \n",
       "2              02139                            3                   2   \n",
       "3              02139                            2                   3   \n",
       "4              60091                            3                   4   \n",
       "5              02139                            3                   5   \n",
       "\n",
       "             YEAR(join_date)  MONTH(join_date)  DAY(join_date)  \\\n",
       "customer_id                                                      \n",
       "1                       2008                 1               1   \n",
       "2                       2008                 2              20   \n",
       "3                       2008                 4              10   \n",
       "4                       2008                 5              30   \n",
       "5                       2008                 7              19   \n",
       "\n",
       "             MODE(transactions.product_id) MODE(sessions.device)  \\\n",
       "customer_id                                                        \n",
       "1                                        3               desktop   \n",
       "2                                        1                mobile   \n",
       "3                                        5               desktop   \n",
       "4                                        4               desktop   \n",
       "5                                        2                tablet   \n",
       "\n",
       "             SUM(transactions.amount)  STD(transactions.amount)  \\\n",
       "customer_id                                                       \n",
       "1                            10236.77                 42.837081   \n",
       "2                             9118.81                 43.382936   \n",
       "3                             5758.24                 40.387654   \n",
       "4                             8205.28                 42.047038   \n",
       "5                             4571.37                 43.028739   \n",
       "\n",
       "                              ...                     \\\n",
       "customer_id                   ...                      \n",
       "1                             ...                      \n",
       "2                             ...                      \n",
       "3                             ...                      \n",
       "4                             ...                      \n",
       "5                             ...                      \n",
       "\n",
       "             NUM_UNIQUE(sessions.MONTH(session_start))  \\\n",
       "customer_id                                              \n",
       "1                                                    1   \n",
       "2                                                    1   \n",
       "3                                                    1   \n",
       "4                                                    1   \n",
       "5                                                    1   \n",
       "\n",
       "             SUM(sessions.MIN(transactions.amount))  \\\n",
       "customer_id                                           \n",
       "1                                            169.77   \n",
       "2                                            114.85   \n",
       "3                                             64.98   \n",
       "4                                             83.53   \n",
       "5                                             73.09   \n",
       "\n",
       "             SKEW(sessions.STD(transactions.amount))  \\\n",
       "customer_id                                            \n",
       "1                                           0.062233   \n",
       "2                                          -0.121708   \n",
       "3                                          -1.523536   \n",
       "4                                          -0.269755   \n",
       "5                                           0.943485   \n",
       "\n",
       "             MAX(sessions.SUM(transactions.amount))  \\\n",
       "customer_id                                           \n",
       "1                                           1248.67   \n",
       "2                                           1461.66   \n",
       "3                                           2054.32   \n",
       "4                                           1609.13   \n",
       "5                                           1356.60   \n",
       "\n",
       "             MEAN(sessions.MAX(transactions.amount))  \\\n",
       "customer_id                                            \n",
       "1                                          143.74700   \n",
       "2                                          139.85375   \n",
       "3                                          135.16400   \n",
       "4                                          140.04500   \n",
       "5                                          141.50000   \n",
       "\n",
       "             STD(sessions.SUM(transactions.amount))  \\\n",
       "customer_id                                           \n",
       "1                                        185.456436   \n",
       "2                                        246.236871   \n",
       "3                                        526.648290   \n",
       "4                                        345.176925   \n",
       "5                                        229.234047   \n",
       "\n",
       "             MEAN(sessions.STD(transactions.amount))  \\\n",
       "customer_id                                            \n",
       "1                                          43.056690   \n",
       "2                                          42.876605   \n",
       "3                                          38.539577   \n",
       "4                                          41.730640   \n",
       "5                                          42.696139   \n",
       "\n",
       "             MIN(sessions.MAX(transactions.amount))  \\\n",
       "customer_id                                           \n",
       "1                                            131.83   \n",
       "2                                            124.29   \n",
       "3                                            111.02   \n",
       "4                                            108.05   \n",
       "5                                            132.72   \n",
       "\n",
       "             MODE(sessions.DAY(session_start))  \\\n",
       "customer_id                                      \n",
       "1                                            1   \n",
       "2                                            1   \n",
       "3                                            1   \n",
       "4                                            1   \n",
       "5                                            1   \n",
       "\n",
       "             SUM(sessions.MEAN(transactions.amount))  \n",
       "customer_id                                           \n",
       "1                                         791.976505  \n",
       "2                                         596.243506  \n",
       "3                                         369.770121  \n",
       "4                                         584.673126  \n",
       "5                                         313.448942  \n",
       "\n",
       "[5 rows x 69 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_matrix, features_defs = ft.dfs(entityset=es, target_entity=\"customers\")\n",
    "feature_matrix.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><code>features_defs</code>から以下のような特徴を自動的に生成していることがわかります。　ここで、<code>MODE</code>や<code>SUM</code>、<code>STD</code>というのが primitive です。つまり、<code>SUM(transactions.amount)</code>は<code>transactions</code>テーブルの<code>amount</code>列の合計を特徴として加えるということを意味しています。</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Feature: zip_code>, <Feature: NUM_UNIQUE(sessions.device)>, <Feature: WEEKDAY(join_date)>, <Feature: YEAR(join_date)>, <Feature: MONTH(join_date)>, <Feature: DAY(join_date)>, <Feature: MODE(transactions.product_id)>, <Feature: MODE(sessions.device)>, <Feature: SUM(transactions.amount)>, <Feature: STD(transactions.amount)>, <Feature: COUNT(transactions)>, <Feature: SKEW(transactions.amount)>, <Feature: MAX(transactions.amount)>, <Feature: NUM_UNIQUE(transactions.product_id)>, <Feature: COUNT(sessions)>, <Feature: MEAN(transactions.amount)>, <Feature: MIN(transactions.amount)>, <Feature: MAX(sessions.SKEW(transactions.amount))>, <Feature: STD(sessions.MIN(transactions.amount))>, <Feature: STD(sessions.MAX(transactions.amount))>, <Feature: NUM_UNIQUE(sessions.DAY(session_start))>, <Feature: MIN(sessions.SUM(transactions.amount))>, <Feature: MAX(sessions.COUNT(transactions))>, <Feature: STD(sessions.MEAN(transactions.amount))>, <Feature: MODE(sessions.MODE(transactions.product_id))>, <Feature: SUM(sessions.SKEW(transactions.amount))>, <Feature: MODE(sessions.WEEKDAY(session_start))>, <Feature: NUM_UNIQUE(sessions.MODE(transactions.product_id))>, <Feature: SKEW(sessions.MEAN(transactions.amount))>, <Feature: MIN(sessions.NUM_UNIQUE(transactions.product_id))>, <Feature: MODE(sessions.MONTH(session_start))>, <Feature: SKEW(sessions.MAX(transactions.amount))>, <Feature: STD(sessions.SKEW(transactions.amount))>, <Feature: MAX(sessions.NUM_UNIQUE(transactions.product_id))>, <Feature: SUM(sessions.MAX(transactions.amount))>, <Feature: NUM_UNIQUE(sessions.WEEKDAY(session_start))>, <Feature: MEAN(sessions.MEAN(transactions.amount))>, <Feature: MEAN(sessions.NUM_UNIQUE(transactions.product_id))>, <Feature: MIN(sessions.COUNT(transactions))>, <Feature: STD(sessions.NUM_UNIQUE(transactions.product_id))>, <Feature: SKEW(sessions.SUM(transactions.amount))>, <Feature: MEAN(sessions.MIN(transactions.amount))>, <Feature: MEAN(sessions.SKEW(transactions.amount))>, <Feature: MIN(sessions.STD(transactions.amount))>, <Feature: MAX(sessions.MIN(transactions.amount))>, <Feature: STD(sessions.COUNT(transactions))>, <Feature: MEAN(sessions.SUM(transactions.amount))>, <Feature: SUM(sessions.NUM_UNIQUE(transactions.product_id))>, <Feature: SKEW(sessions.COUNT(transactions))>, <Feature: SKEW(sessions.MIN(transactions.amount))>, <Feature: SUM(sessions.STD(transactions.amount))>, <Feature: MODE(sessions.YEAR(session_start))>, <Feature: MAX(sessions.MEAN(transactions.amount))>, <Feature: MIN(sessions.MEAN(transactions.amount))>, <Feature: MEAN(sessions.COUNT(transactions))>, <Feature: SKEW(sessions.NUM_UNIQUE(transactions.product_id))>, <Feature: NUM_UNIQUE(sessions.YEAR(session_start))>, <Feature: MIN(sessions.SKEW(transactions.amount))>, <Feature: MAX(sessions.STD(transactions.amount))>, <Feature: NUM_UNIQUE(sessions.MONTH(session_start))>, <Feature: SUM(sessions.MIN(transactions.amount))>, <Feature: SKEW(sessions.STD(transactions.amount))>, <Feature: MAX(sessions.SUM(transactions.amount))>, <Feature: MEAN(sessions.MAX(transactions.amount))>, <Feature: STD(sessions.SUM(transactions.amount))>, <Feature: MEAN(sessions.STD(transactions.amount))>, <Feature: MIN(sessions.MAX(transactions.amount))>, <Feature: MODE(sessions.DAY(session_start))>, <Feature: SUM(sessions.MEAN(transactions.amount))>]\n"
     ]
    }
   ],
   "source": [
    "print(features_defs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>featuretoolsではこのように特徴を生成することで、特徴エンジニアリングにかかる時間を軽減することができます。</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "<span id=\"automlのソフトウェア\" class=\"fragment\"></span><a href=\"#automl%E3%81%AE%E3%82%BD%E3%83%95%E3%83%88%E3%82%A6%E3%82%A7%E3%82%A2\"><i class=\"fa fa-link\"></i></a>AutoMLのソフトウェア</h1>\n",
    "\n",
    "<p>AutoMLのための数多くのソフトウェアやサービスがすでに公開されています。ここでは、OSSと非OSSという2つの区分で分けると以下に列挙するソフトウェアがあります。</p>\n",
    "\n",
    "<p>OSS</p>\n",
    "\n",
    "<ul>\n",
    "<li><a href=\"https://www.cs.ubc.ca/labs/beta/Projects/autoweka/\" rel=\"nofollow noopener\" target=\"_blank\">auto-Weka</a></li>\n",
    "<li><a href=\"https://automl.github.io/auto-sklearn/stable/\" rel=\"nofollow noopener\" target=\"_blank\">auto-sklearn</a></li>\n",
    "<li><a href=\"https://autokeras.com/\" rel=\"nofollow noopener\" target=\"_blank\">Auto-Keras</a></li>\n",
    "<li><a href=\"http://epistasislab.github.io/tpot/\" rel=\"nofollow noopener\" target=\"_blank\">TPOT</a></li>\n",
    "<li><a href=\"http://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html\" rel=\"nofollow noopener\" target=\"_blank\">H2O AutoML</a></li>\n",
    "<li><a href=\"https://github.com/joeddav/devol\" rel=\"nofollow noopener\" target=\"_blank\">devol</a></li>\n",
    "<li><a href=\"https://github.com/pfnet/optuna\" rel=\"nofollow noopener\" target=\"_blank\">optuna</a></li>\n",
    "</ul>\n",
    "\n",
    "<p>非OSS</p>\n",
    "\n",
    "<ul>\n",
    "<li><a href=\"https://www.datarobot.com/\" rel=\"nofollow noopener\" target=\"_blank\">DataRobot</a></li>\n",
    "<li><a href=\"https://www.h2o.ai/products/h2o-driverless-ai/\" rel=\"nofollow noopener\" target=\"_blank\">H2o.ai Driverless AI</a></li>\n",
    "<li><a href=\"https://cloud.google.com/automl/\" rel=\"nofollow noopener\" target=\"_blank\">Google Cloud AutoML</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "<span id=\"automlの将来\" class=\"fragment\"></span><a href=\"#automl%E3%81%AE%E5%B0%86%E6%9D%A5\"><i class=\"fa fa-link\"></i></a>AutoMLの将来</h1>\n",
    "\n",
    "<p>最後はAutoMLの将来についての私見です。まず、AutoMLは将来的にはデータクリーニングのプロセスも扱えるようになるのではないかと考えています。たとえば、テキストのような非構造化データを分析にすぐに使えるようにテーブルデータに変換するといったことです。次に、大規模データにスケールするような方法が出てくるでしょう。Cloud AutoMLを試してみるとわかるのですが、サンプルの小さなデータに対してでさえ計算時間が結構かかります。将来的にはいわゆるビッグデータに対しても使えるようになるでしょう。最後に、性能が人間を上回るようになるでしょう。現在でも一部のデータセットでは人間に匹敵する性能を出していますが、将来的には人間が考えつかないような特徴であるとかネットワークアーキテクチャを生み出せるようになるでしょう。そういう意味では、Alpha Goに似たところはあるかもしれません。</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "<span id=\"参考資料\" class=\"fragment\"></span><a href=\"#%E5%8F%82%E8%80%83%E8%B3%87%E6%96%99\"><i class=\"fa fa-link\"></i></a>参考資料</h1>\n",
    "\n",
    "<p><strong>特徴エンジニアリング</strong></p>\n",
    "\n",
    "<ul>\n",
    "<li><a href=\"https://towardsdatascience.com/why-automated-feature-engineering-will-change-the-way-you-do-machine-learning-5c15bf188b96?gi=2978fbc2ade3\" rel=\"nofollow noopener\" target=\"_blank\">Why Automated Feature Engineering Will Change the Way You Do Machine Learning</a></li>\n",
    "<li><a href=\"https://blog.featurelabs.com/deep-feature-synthesis/\" rel=\"nofollow noopener\" target=\"_blank\">Deep Feature Synthesis: How Automated Feature Engineering Works</a></li>\n",
    "<li><a href=\"https://blog.datarobot.com/automated-feature-engineering\" rel=\"nofollow noopener\" target=\"_blank\">Automated Feature Engineering</a></li>\n",
    "</ul>\n",
    "\n",
    "<p><strong>ニューラルアーキテクチャサーチ</strong></p>\n",
    "\n",
    "<ul>\n",
    "<li><a href=\"http://rll.berkeley.edu/deeprlcoursesp17/docs/quoc_barret.pdf\" rel=\"nofollow noopener\" target=\"_blank\">Neural Architecture Search with Reinforcement Learning</a></li>\n",
    "<li><a href=\"https://arxiv.org/abs/1707.07012\" rel=\"nofollow noopener\" target=\"_blank\">Learning Transferable Architectures for Scalable Image Recognition</a></li>\n",
    "<li><a href=\"https://arxiv.org/abs/1802.03268\" rel=\"nofollow noopener\" target=\"_blank\">Efficient Neural Architecture Search via Parameter Sharing</a></li>\n",
    "<li><a href=\"https://towardsdatascience.com/everything-you-need-to-know-about-automl-and-neural-architecture-search-8db1863682bf\" rel=\"nofollow noopener\" target=\"_blank\">Everything you need to know about AutoML and Neural Architecture Search</a></li>\n",
    "<li><a href=\"https://medium.com/aifrontiers/understand-automl-and-neural-architecture-search-4260a0942116\" rel=\"nofollow noopener\" target=\"_blank\">Understanding AutoML and Neural Architecture Search</a></li>\n",
    "<li><a href=\"https://www.fast.ai/2018/07/16/auto-ml2/\" rel=\"nofollow noopener\" target=\"_blank\">An Opinionated Introduction to AutoML and Neural Architecture Search</a></li>\n",
    "<li><a href=\"https://www.fast.ai/2018/07/12/auto-ml-1/\" rel=\"nofollow noopener\" target=\"_blank\">What do machine learning practitioners actually do?</a></li>\n",
    "</ul>\n",
    "\n",
    "<p><strong>ハイパーパラメータチューニング</strong></p>\n",
    "\n",
    "<ul>\n",
    "<li><a href=\"http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf\" rel=\"nofollow noopener\" target=\"_blank\">Random Search for Hyper-Parameter Optimization</a></li>\n",
    "<li><a href=\"https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f\" rel=\"nofollow noopener\" target=\"_blank\">A Conceptual Explanation of Bayesian Hyperparameter Optimization for Machine Learning</a></li>\n",
    "<li><a href=\"https://www.slideshare.net/greetech/ss-110811527\" rel=\"nofollow noopener\" target=\"_blank\">機械学習モデルのハイパパラメータ最適化</a></li>\n",
    "<li><a href=\"https://www.slideshare.net/hoxo_m/ss-77421091\" rel=\"nofollow noopener\" target=\"_blank\">機械学習のためのベイズ最適化入門</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
